{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/llk010502/RL_lab/blob/main/RL_playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXsKbv6qfd3k"
      },
      "outputs": [],
      "source": [
        "! pip install trl\n",
        "! pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "e9ea686ff7164777bfb9a80d322fa3d1",
            "c6247212a1f9408d8413ee900ec3b6f2",
            "4e45ac01c7f94096ab3e6f152bd1daa9",
            "34f0bb22e39c45708eeecdaf29c769c1",
            "fd7c795acb7744299211b220db3bbea1",
            "12f042afff8944d5bdcb0d4d481e060d",
            "58445a7b60d7452ca7b6dbfe506a41a8",
            "81344998d8d34b0d822377d0e47a3e70",
            "b54b8123305f4859bc934357261286ed",
            "5f5101f3228043049f1c47ed06f965f9",
            "94d940b264984e5e86775b2374a1c673",
            "824976014c00416d9ddd8392db076872",
            "06f22992bfc1457ebebaee52bc8773d1",
            "0aa1b977ce3c4ec09f48be7ab29928d7",
            "86c852f5661c4d76904247328fc21f1d",
            "a046a33c08b0433ea66f126e71e17d0e",
            "9a70f6bd453e42f995a0ef40c6f22ae2",
            "f26c3fbda5654889a0f061b854ff2063",
            "10c0fefc702c4f7e808a11a010f4951d",
            "803fe311a20a462b8fcb6d5507cc36c8"
          ]
        },
        "id": "DlnaPUbl4mJF",
        "outputId": "3f34bb75-4d10-4c58-9d37-87f41b4dab6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9ea686ff7164777bfb9a80d322fa3d1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRL--GRPO\n",
        "* Load model: 6.0 / 40.0 GB\n",
        "* During training: 31.9-39.4 / 40.0 GB"
      ],
      "metadata": {
        "id": "GuGQlZBdWsFF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYihViFsdx9A",
        "outputId": "776c0b92-035c-4d75-aefa-7151c1de4c7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "from peft import (\n",
        "    TaskType,\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        ")\n",
        "\n",
        "# prepare dataset\n",
        "dataset = load_dataset(\"trl-lib/tldr\", split=\"train\").select(range(500))\n",
        "\n",
        "# define one or more reward functions\n",
        "# simple example: length constraint\n",
        "def reward_len(completions, **kwargs):\n",
        "    return [-abs(20 - len(c)) for c in completions]\n",
        "\n",
        "# load model\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"cuda\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
        "        \"up_proj\",\"gate_proj\",\"down_proj\"\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training configuration\n",
        "training_args = GRPOConfig(\n",
        "    run_name = \"Llama-3.2-1B-Instruct-GRPO\",\n",
        "    output_dir=\"Llama-3.2-1B-Instruct-GRPO\",\n",
        "    learning_rate=1e-4,\n",
        "    lr_scheduler_type='cosine',\n",
        "    logging_steps=5,\n",
        "    bf16=True,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    num_generations=4, # number of generations for each query\n",
        "    max_prompt_length=256,\n",
        "    max_completion_length=786,\n",
        "    num_train_epochs=1,\n",
        "    save_steps=100,\n",
        "    report_to=\"wandb\",\n",
        "    max_steps=10\n",
        ")\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=reward_len,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    #peft_config=peft_config\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "AWY65ddN7zjZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "971ff94a-1cf7-4c04-a76a-5a5188b372cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mll3713\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250521_172934-jv5ajhgg</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ll3713/huggingface/runs/jv5ajhgg' target=\"_blank\">Llama-3.2-1B-Instruct-GRPO</a></strong> to <a href='https://wandb.ai/ll3713/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ll3713/huggingface' target=\"_blank\">https://wandb.ai/ll3713/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ll3713/huggingface/runs/jv5ajhgg' target=\"_blank\">https://wandb.ai/ll3713/huggingface/runs/jv5ajhgg</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 03:51, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.123400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.044900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10, training_loss=0.08412548154592514, metrics={'train_runtime': 252.4023, 'train_samples_per_second': 0.158, 'train_steps_per_second': 0.04, 'total_flos': 0.0, 'train_loss': 0.08412548154592514})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRL--LoRA_GRPO\n",
        "* Load model: 6.0 / 40.0 GB\n",
        "* Initialize training: 23.6 / 40.0 GB\n"
      ],
      "metadata": {
        "id": "r9uvHH2fYaZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training configuration\n",
        "training_args = GRPOConfig(\n",
        "    run_name = \"Llama-3.2-1B-Instruct-GRPO-LoRA\",\n",
        "    output_dir=\"Llama-3.2-1B-Instruct-GRPO-LoRA\",\n",
        "    learning_rate=1e-4,\n",
        "    lr_scheduler_type='cosine',\n",
        "    logging_steps=5,\n",
        "    bf16=True,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    num_generations=4, # number of generations for each query\n",
        "    max_prompt_length=256,\n",
        "    max_completion_length=786,\n",
        "    num_train_epochs=1,\n",
        "    save_steps=100,\n",
        "    report_to=\"wandb\",\n",
        "    max_steps=10\n",
        ")\n",
        "\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=reward_len,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config # activate peft\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "gVT8ZiMxXGvX",
        "outputId": "6c3dd2b6-bb92-4499-82f6-a7ae408c3c0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mll3713\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250521_173459-7oi9z2xg</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ll3713/huggingface/runs/7oi9z2xg' target=\"_blank\">Llama-3.2-1B-Instruct-GRPO-LoRA</a></strong> to <a href='https://wandb.ai/ll3713/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ll3713/huggingface' target=\"_blank\">https://wandb.ai/ll3713/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ll3713/huggingface/runs/7oi9z2xg' target=\"_blank\">https://wandb.ai/ll3713/huggingface/runs/7oi9z2xg</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 02:15, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.379300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.414900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10, training_loss=0.39709086418151857, metrics={'train_runtime': 170.2145, 'train_samples_per_second': 0.235, 'train_steps_per_second': 0.059, 'total_flos': 0.0, 'train_loss': 0.39709086418151857})"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## verl--PPO\n",
        "\t•\tModel weights: approximately 0.5*4B*3 = 6 GB\n",
        "\t•\tOptimizer states: approximately 0.5*4B*2*2 = 8 GB\n",
        "\t•\tGradient buffers: approximately 4 GB\n",
        "\t•\tFSDP & engine buffers: approximately 5–7 GB\n",
        "\t•\tRL caches (log-probs, values, advantages): approximately 3–4 GB"
      ],
      "metadata": {
        "id": "oi2vJx5NwIkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/volcengine/verl.git"
      ],
      "metadata": {
        "id": "VCZUkGNtCFpu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20394820-9e2e-4454-9d10-209ca278ef48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'verl'...\n",
            "remote: Enumerating objects: 10432, done.\u001b[K\n",
            "remote: Counting objects: 100% (447/447), done.\u001b[K\n",
            "remote: Compressing objects: 100% (333/333), done.\u001b[K\n",
            "remote: Total 10432 (delta 326), reused 114 (delta 114), pack-reused 9985 (from 3)\u001b[K\n",
            "Receiving objects: 100% (10432/10432), 5.41 MiB | 11.53 MiB/s, done.\n",
            "Resolving deltas: 100% (7133/7133), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install verl fastapi uvicorn flash-attn vllm"
      ],
      "metadata": {
        "id": "mHdznUaJwaFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U datasets fsspec huggingface_hub"
      ],
      "metadata": {
        "id": "d9HmWiuW1gbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd verl/\n",
        "! python examples/data_preprocess/gsm8k.py --local_dir ~/data/gsm8k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vv-EcqfAyCZL",
        "outputId": "c6740d5b-4c3c-4702-d083-150c6494fe2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/verl\n",
            "Creating parquet from Arrow format: 100% 8/8 [00:00<00:00, 166.40ba/s]\n",
            "Creating parquet from Arrow format: 100% 2/2 [00:00<00:00, 269.08ba/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test model loading\n",
        "! python -c \"import transformers; transformers.pipeline('text-generation', model='Qwen/Qwen2.5-0.5B-Instruct')\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czOU5ukf1FhS",
        "outputId": "f43f4d1f-1636-4b79-99ae-68c676726c93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-25 21:27:43.742180: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-25 21:27:43.758707: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1748208463.779643    3443 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1748208463.785977    3443 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-25 21:27:43.806547: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "config.json: 100% 659/659 [00:00<00:00, 5.73MB/s]\n",
            "model.safetensors: 100% 988M/988M [00:02<00:00, 383MB/s]\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "generation_config.json: 100% 242/242 [00:00<00:00, 1.92MB/s]\n",
            "tokenizer_config.json: 100% 7.30k/7.30k [00:00<00:00, 38.1MB/s]\n",
            "vocab.json: 100% 2.78M/2.78M [00:00<00:00, 15.9MB/s]\n",
            "merges.txt: 100% 1.67M/1.67M [00:00<00:00, 4.98MB/s]\n",
            "tokenizer.json: 100% 7.03M/7.03M [00:00<00:00, 15.1MB/s]\n",
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! PYTHONUNBUFFERED=1 python3 -m verl.trainer.main_ppo \\\n",
        " data.train_files=$HOME/data/gsm8k/train.parquet \\\n",
        " data.val_files=$HOME/data/gsm8k/test.parquet \\\n",
        " data.train_batch_size=256 \\\n",
        " data.max_prompt_length=512 \\\n",
        " data.max_response_length=256 \\\n",
        " actor_rollout_ref.model.path=Qwen/Qwen2.5-0.5B-Instruct \\\n",
        " actor_rollout_ref.actor.optim.lr=1e-6 \\\n",
        " actor_rollout_ref.model.use_remove_padding=True \\\n",
        " actor_rollout_ref.actor.ppo_mini_batch_size=64 \\\n",
        " actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4 \\\n",
        " actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8 \\\n",
        " actor_rollout_ref.rollout.tensor_model_parallel_size=1 \\\n",
        " actor_rollout_ref.rollout.gpu_memory_utilization=0.4 \\\n",
        " actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 \\\n",
        " critic.optim.lr=1e-5 \\\n",
        " critic.model.path=Qwen/Qwen2.5-0.5B-Instruct \\\n",
        " critic.ppo_micro_batch_size_per_gpu=4 \\\n",
        " algorithm.kl_ctrl.kl_coef=0.001 \\\n",
        " trainer.logger=['console'] \\\n",
        " trainer.val_before_train=False \\\n",
        " trainer.default_hdfs_dir=null \\\n",
        " trainer.n_gpus_per_node=1 \\\n",
        " trainer.nnodes=1 \\\n",
        " trainer.save_freq=10 \\\n",
        " trainer.test_freq=10 \\\n",
        " trainer.total_epochs=1 2>&1 | tee verl_demo.log"
      ],
      "metadata": {
        "id": "gLwk78u82Aeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### verl--GRPO (RAM usage: 14-16GB)\n",
        "\t•\tModel weights: approximately 3 GB -- use bfloat16 this time(2B per parameter)\n",
        "\t•\tOptimizer states: approximately 8 GB\n",
        "\t•\tGradient buffers: approximately 2 GB\n",
        "\t•\tFSDP & engine buffers: approximately 4 GB\n",
        "\t•\tRL caches (log-probs, values, advantages): approximately 0.5 GB\n",
        "\n",
        "\n",
        "#### parameter explanation\n",
        "* choose GRPO algo: algorithm.adv_estimator=grpo\n",
        "* actor-level kl penalty: actor_rollout_ref.actor.use_kl_loss=True\n",
        "  (**Reparameterization need extra calculation (low-variance trick)**)\n",
        "* not consider tensor parallel: actor_rollout_ref.rollout.tensor_model_parallel_size=1\n",
        "* GRPO trajectory group: actor_rollout_ref.rollout.n=5\n",
        "  "
      ],
      "metadata": {
        "id": "kHgEEWYcWRPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Customer reward function:\n",
        " custom_reward_function.path=\"/content/verl/reward_len.py\" \\"
      ],
      "metadata": {
        "id": "bwaYOktbK0-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# customize reward function\n",
        "reward_code = '''\n",
        "def compute_score(data_source, solution_str, ground_truth, extra_info=None):\n",
        "    # reward generation length that is close to 20\n",
        "    return -abs(20 - len(solution_str))\n",
        "'''\n",
        "\n",
        "with open(\"reward_len.py\", \"w\") as f:\n",
        "    f.write(reward_code)"
      ],
      "metadata": {
        "id": "-eTQT6KCCR3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "reward_path = os.path.abspath(\"reward_len.py\")\n",
        "print(\"Absolute reward path:\", reward_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_cR8IObJzTY",
        "outputId": "2e67e7b2-89b1-425e-cda5-fb2dab9f159f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Absolute reward path: /content/verl/reward_len.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python3 -m verl.trainer.main_ppo \\\n",
        "    algorithm.adv_estimator=grpo \\\n",
        "    data.train_files=$HOME/data/gsm8k/train.parquet \\\n",
        "    data.val_files=$HOME/data/gsm8k/test.parquet \\\n",
        "    data.train_batch_size=256 \\\n",
        "    data.max_prompt_length=512 \\\n",
        "    data.max_response_length=256 \\\n",
        "    actor_rollout_ref.model.path=Qwen/Qwen2.5-0.5B-Instruct \\\n",
        "    actor_rollout_ref.actor.optim.lr=1e-6 \\\n",
        "    actor_rollout_ref.model.use_remove_padding=True \\\n",
        "    actor_rollout_ref.actor.ppo_mini_batch_size=64 \\\n",
        "    actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4 \\\n",
        "    actor_rollout_ref.actor.use_kl_loss=True \\\n",
        "    actor_rollout_ref.actor.kl_loss_coef=0.001 \\\n",
        "    actor_rollout_ref.actor.kl_loss_type=low_var_kl \\\n",
        "    actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8 \\\n",
        "    actor_rollout_ref.rollout.tensor_model_parallel_size=1 \\\n",
        "    actor_rollout_ref.rollout.gpu_memory_utilization=0.4 \\\n",
        "    actor_rollout_ref.rollout.n=5 \\\n",
        "    actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 \\\n",
        "    algorithm.kl_ctrl.kl_coef=0.001 \\\n",
        "    custom_reward_function.path=\"/content/verl/reward_len.py\" \\\n",
        "    trainer.critic_warmup=0 \\\n",
        "    trainer.logger=['console'] \\\n",
        "    trainer.val_before_train=False \\\n",
        "    trainer.n_gpus_per_node=1 \\\n",
        "    trainer.nnodes=1 \\\n",
        "    trainer.save_freq=10 \\\n",
        "    trainer.test_freq=10 \\\n",
        "    trainer.total_epochs=1 2>&1 | tee verl_demo.log"
      ],
      "metadata": {
        "id": "wmbv_bat3cKw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b43d639-c5c4-4fef-b85c-236e48e25f5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-25 21:32:10,654\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m {'actor_rollout_ref': {'actor': {'checkpoint': {'contents': ['model',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                                              'optimizer',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                                              'extra']},\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'clip_ratio': 0.2,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'clip_ratio_c': 3.0,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'clip_ratio_high': 0.2,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'clip_ratio_low': 0.2,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'entropy_coeff': 0,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'fsdp_config': {'fsdp_size': -1,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                                  'offload_policy': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                                  'optimizer_offload': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                                  'param_offload': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                                  'reshard_after_forward': True,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                                  'wrap_policy': {'min_num_params': 0}},\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'grad_clip': 1.0,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'kl_loss_coef': 0.001,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'kl_loss_type': 'low_var_kl',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'loss_agg_mode': 'token-mean',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'optim': {'lr': 1e-06,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                            'lr_warmup_steps': -1,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                            'lr_warmup_steps_ratio': 0.0,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                            'min_lr_ratio': 0.0,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                            'num_cycles': 0.5,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                            'total_training_steps': -1,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                            'warmup_style': 'constant',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                            'weight_decay': 0.01},\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'ppo_epochs': 1,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'ppo_max_token_len_per_gpu': 16384,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'ppo_micro_batch_size': None,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'ppo_micro_batch_size_per_gpu': 4,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'ppo_mini_batch_size': 64,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'shuffle': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'strategy': 'fsdp',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'ulysses_sequence_parallel_size': 1,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'use_dynamic_bsz': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'use_kl_loss': True,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'use_torch_compile': True},\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                        'hybrid_engine': True,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                        'model': {'enable_activation_offload': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'enable_gradient_checkpointing': True,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'external_lib': None,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'override_config': {},\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'path': 'Qwen/Qwen2.5-0.5B-Instruct',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'trust_remote_code': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'use_fused_kernels': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'use_liger': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                  'use_remove_padding': False},\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                        'ref': {'fsdp_config': {'param_offload': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                                'reshard_after_forward': True,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                                'wrap_policy': {'min_num_params': 0}},\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                'log_prob_max_token_len_per_gpu': 16384,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                'log_prob_micro_batch_size': None,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                'log_prob_micro_batch_size_per_gpu': 4,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                'log_prob_use_dynamic_bsz': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                'strategy': 'fsdp',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                'ulysses_sequence_parallel_size': 1,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                'use_torch_compile': True},\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                        'rollout': {'chat_scheduler': None,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'disable_log_stats': True,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'do_sample': True,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'dtype': 'bfloat16',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'enable_chunked_prefill': True,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'enforce_eager': True,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'engine_kwargs': {'sglang': {'attention_backend': None},\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                                      'vllm': {'swap_space': None}},\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'free_cache_engine': True,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'gpu_memory_utilization': 0.4,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'ignore_eos': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'load_format': 'dummy_dtensor',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'log_prob_max_token_len_per_gpu': 16384,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'log_prob_micro_batch_size': None,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'log_prob_micro_batch_size_per_gpu': 8,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'log_prob_use_dynamic_bsz': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'max_model_len': None,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'max_num_batched_tokens': 8192,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'max_num_seqs': 1024,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'mode': 'sync',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'multi_turn': {'enable': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                                   'format': 'chatml',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                                   'max_turns': None,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                                   'tool_config_path': None},\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'n': 5,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'name': 'vllm',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'prompt_length': 512,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'response_length': 256,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'temperature': 1.0,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'tensor_model_parallel_size': 1,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'top_k': -1,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'top_p': 1,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'use_fire_sampling': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                    'val_kwargs': {'do_sample': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                                   'n': 1,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                                   'temperature': 0,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                                   'top_k': -1,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                                   'top_p': 1.0}}},\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m  'algorithm': {'adv_estimator': 'grpo',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                'gamma': 1.0,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                'kl_ctrl': {'horizon': 10000,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                            'kl_coef': 0.001,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                            'target_kl': 0.1,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                            'type': 'fixed'},\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                'kl_penalty': 'kl',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                'lam': 1.0,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                'norm_adv_by_std_in_grpo': True,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                'use_kl_in_reward': False},\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m  'critic': {'checkpoint': {'contents': ['model', 'optimizer', 'extra']},\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m             'cliprange_value': 0.5,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m             'forward_max_token_len_per_gpu': 32768,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m             'forward_micro_batch_size': None,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m             'forward_micro_batch_size_per_gpu': None,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m             'grad_clip': 1.0,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m             'loss_agg_mode': 'token-mean',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m             'model': {'enable_activation_offload': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                       'enable_gradient_checkpointing': True,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                       'external_lib': None,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                       'fsdp_config': {'fsdp_size': -1,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                       'offload_policy': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                       'optimizer_offload': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                       'param_offload': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                       'reshard_after_forward': True,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                       'wrap_policy': {'min_num_params': 0}},\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                       'override_config': {},\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                       'path': '~/models/deepseek-llm-7b-chat',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                       'tokenizer_path': 'Qwen/Qwen2.5-0.5B-Instruct',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                       'trust_remote_code': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                       'use_remove_padding': False},\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m             'optim': {'lr': 1e-05,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                       'lr_warmup_steps_ratio': 0.0,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                       'min_lr_ratio': None,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                       'total_training_steps': -1,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                       'warmup_style': 'constant',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                       'weight_decay': 0.01},\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m             'ppo_epochs': 1,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m             'ppo_max_token_len_per_gpu': 32768,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m             'ppo_micro_batch_size': None,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m             'ppo_micro_batch_size_per_gpu': None,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m             'ppo_mini_batch_size': 64,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m             'rollout_n': 5,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m             'shuffle': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m             'strategy': 'fsdp',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m             'ulysses_sequence_parallel_size': 1,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m             'use_dynamic_bsz': False},\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m  'custom_reward_function': {'name': 'compute_score', 'path': None},\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m  'data': {'custom_cls': {'name': None, 'path': None},\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m           'filter_overlong_prompts': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m           'filter_overlong_prompts_workers': 1,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m           'image_key': 'images',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m           'max_prompt_length': 512,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m           'max_response_length': 256,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m           'prompt_key': 'prompt',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m           'return_full_prompt': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m           'return_raw_chat': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m           'return_raw_input_ids': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m           'reward_fn_key': 'data_source',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m           'shuffle': True,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m           'tokenizer': None,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m           'train_batch_size': 256,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m           'train_files': '/root/data/gsm8k/train.parquet',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m           'truncation': 'error',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m           'val_batch_size': None,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m           'val_files': '/root/data/gsm8k/test.parquet',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m           'video_key': 'videos'},\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m  'ray_init': {'num_cpus': None},\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m  'reward_model': {'enable': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                   'forward_max_token_len_per_gpu': 32768,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                   'launch_reward_fn_async': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                   'max_length': None,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                   'micro_batch_size': None,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                   'micro_batch_size_per_gpu': None,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                   'model': {'external_lib': None,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                             'fsdp_config': {'fsdp_size': -1,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                             'param_offload': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                             'reshard_after_forward': True,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                                             'wrap_policy': {'min_num_params': 0}},\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                             'input_tokenizer': \n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m 'Qwen/Qwen2.5-0.5B-Instruct',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                             'path': '~/models/FsfairX-LLaMA3-RM-v0.1',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                             'trust_remote_code': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                             'use_fused_kernels': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                             'use_remove_padding': False},\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                   'reward_manager': 'naive',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                   'sandbox_fusion': {'max_concurrent': 64, 'url': None},\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                   'strategy': 'fsdp',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                   'ulysses_sequence_parallel_size': 1,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m                   'use_dynamic_bsz': False},\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m  'trainer': {'balance_batch': True,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m              'critic_warmup': 0,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m              'default_hdfs_dir': None,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m              'default_local_dir': 'checkpoints/verl_examples/gsm8k',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m              'del_local_ckpt_after_load': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m              'device': 'cuda',\u001b[36m(TaskRunner pid=5907)\u001b[0m 2025-05-25 21:32:18.916605: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m 2025-05-25 21:32:18.936671: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m E0000 00:00:1748208738.957294    5907 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m E0000 00:00:1748208738.962761    5907 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m 2025-05-25 21:32:18.981413: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Generating train split: 0 examples [00:00, ? examples/s]\n",
            "Generating train split: 7473 examples [00:00, 226772.44 examples/s]\n",
            "Generating train split: 1319 examples [00:00, 147190.10 examples/s]\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m DeprecationWarning: `ray.state.available_resources_per_node` is a private attribute and access will be removed in a future Ray version.\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m WARNING:2025-05-25 21:32:24,553:Waiting for register center actor ZJqLQl_register_center to be ready. Elapsed time: 0 seconds out of 300 seconds.\n",
            "\u001b[36m(pid=6106)\u001b[0m 2025-05-25 21:32:29.933642: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=6106)\u001b[0m 2025-05-25 21:32:29.950121: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(pid=6106)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(pid=6106)\u001b[0m E0000 00:00:1748208749.971080    6106 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=6106)\u001b[0m E0000 00:00:1748208749.977481    6106 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(pid=6106)\u001b[0m 2025-05-25 21:32:29.997970: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=6106)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_init_utils.py:444: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_init_utils.py:444: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   warnings.warn(\n",
            "Training Progress:   0%|          | 0/29 [00:00<?, ?it/s]\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_state_dict_utils.py:773: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_state_dict_utils.py:711: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   warnings.warn(\n",
            "\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m              'experiment_name': 'gsm8k',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m              'log_val_generations': 0,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m              'logger': ['console'],\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m              'max_actor_ckpt_to_keep': None,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m              'max_critic_ckpt_to_keep': None,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m              'n_gpus_per_node': 1,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m              'nnodes': 1,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m              'project_name': 'verl_examples',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m              'ray_wait_register_center_timeout': 300,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m              'resume_from_path': None,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m              'resume_mode': 'auto',\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m              'rollout_data_dir': None,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m              'save_freq': 10,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m              'test_freq': 10,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m              'total_epochs': 1,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m              'total_training_steps': None,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m              'val_before_train': False,\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m              'validation_data_dir': None}}\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m Using dataset class: RLHFDataset\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m dataset len: 7473\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m Using dataset class: RLHFDataset\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m dataset len: 1319\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m [validate_config] All configuration checks passed successfully!\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m Size of train dataloader: 29, Size of val dataloader: 1\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m Total training steps: 29\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m colocated worker base class <class 'verl.single_controller.base.worker.Worker'>\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m Model config after override: Qwen2Config {\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"architectures\": [\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m     \"Qwen2ForCausalLM\"\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   ],\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"attention_dropout\": 0.0,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"eos_token_id\": 151645,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"hidden_act\": \"silu\",\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"hidden_size\": 896,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"initializer_range\": 0.02,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"intermediate_size\": 4864,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"max_position_embeddings\": 32768,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"max_window_layers\": 21,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"model_type\": \"qwen2\",\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"num_attention_heads\": 14,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"num_hidden_layers\": 24,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"num_key_value_heads\": 2,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"pad_token_id\": 151643,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"rope_scaling\": null,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"rope_theta\": 1000000.0,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"sliding_window\": 32768,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"tie_word_embeddings\": true,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"transformers_version\": \"4.51.3\",\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"use_cache\": true,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"use_sliding_window\": false,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"vocab_size\": 151936\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m }\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m \n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m Qwen2ForCausalLM contains 494.03M parameters\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m wrap_policy: functools.partial(<function _or_policy at 0x7821b092f6a0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7821b092f560>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m NCCL version 2.21.5+cuda12.4\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m Actor use_remove_padding=False\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m Actor use_fused_kernels=False\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m Model config after override: Qwen2Config {\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"architectures\": [\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m     \"Qwen2ForCausalLM\"\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   ],\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"attention_dropout\": 0.0,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"eos_token_id\": 151645,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"hidden_act\": \"silu\",\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"hidden_size\": 896,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"initializer_range\": 0.02,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"intermediate_size\": 4864,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"max_position_embeddings\": 32768,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"max_window_layers\": 21,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"model_type\": \"qwen2\",\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"num_attention_heads\": 14,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"num_hidden_layers\": 24,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"num_key_value_heads\": 2,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"pad_token_id\": 151643,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"rope_scaling\": null,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"rope_theta\": 1000000.0,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"sliding_window\": 32768,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"tie_word_embeddings\": true,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"transformers_version\": \"4.51.3\",\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"use_cache\": true,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"use_sliding_window\": false,\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   \"vocab_size\": 151936\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m }\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m \n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m Qwen2ForCausalLM contains 494.03M parameters\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m wrap_policy: functools.partial(<function _or_policy at 0x7821b092f6a0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7821b092f560>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m Total steps: 29, num_warmup_steps: 0\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m Actor use_remove_padding=False\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m Actor use_fused_kernels=False\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m WARNING 05-25 21:32:59 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m WARNING 05-25 21:33:00 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x78201a1b1410>\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m WARNING 05-25 21:33:00 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m Using LocalLogger is deprecated. The constructor API will change \n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m Checkpoint tracker file does not exist: %s /content/verl/checkpoints/verl_examples/gsm8k/latest_checkpointed_iteration.txt\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m Training from scratch\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m kwargs: {'n': 5, 'logprobs': 0, 'max_tokens': 256, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}\n",
            "\u001b[36m(TaskRunner pid=5907)\u001b[0m list(reward_extra_infos_dict.keys())=[]\n",
            "Training Progress:   3%|▎         | 1/29 [03:12<1:29:41, 192.19s/it]\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_state_dict_utils.py:773: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_state_dict_utils.py:711: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.\n",
            "\u001b[36m(WorkerDict pid=6106)\u001b[0m   warnings.warn(\n",
            "Training Progress:   7%|▋         | 2/29 [06:19<1:25:03, 189.03s/it]\n",
            "Training Progress:  10%|█         | 3/29 [09:24<1:21:12, 187.40s/it]\n",
            "Training Progress:  14%|█▍        | 4/29 [12:27<1:17:25, 185.82s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vEq0d0yTWKP4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOA0k0MsT3d/jnpt+4D0b/J",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e9ea686ff7164777bfb9a80d322fa3d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_58445a7b60d7452ca7b6dbfe506a41a8"
          }
        },
        "c6247212a1f9408d8413ee900ec3b6f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81344998d8d34b0d822377d0e47a3e70",
            "placeholder": "​",
            "style": "IPY_MODEL_b54b8123305f4859bc934357261286ed",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "4e45ac01c7f94096ab3e6f152bd1daa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_5f5101f3228043049f1c47ed06f965f9",
            "placeholder": "​",
            "style": "IPY_MODEL_94d940b264984e5e86775b2374a1c673",
            "value": ""
          }
        },
        "34f0bb22e39c45708eeecdaf29c769c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_824976014c00416d9ddd8392db076872",
            "style": "IPY_MODEL_06f22992bfc1457ebebaee52bc8773d1",
            "value": true
          }
        },
        "fd7c795acb7744299211b220db3bbea1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_0aa1b977ce3c4ec09f48be7ab29928d7",
            "style": "IPY_MODEL_86c852f5661c4d76904247328fc21f1d",
            "tooltip": ""
          }
        },
        "12f042afff8944d5bdcb0d4d481e060d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a046a33c08b0433ea66f126e71e17d0e",
            "placeholder": "​",
            "style": "IPY_MODEL_9a70f6bd453e42f995a0ef40c6f22ae2",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "58445a7b60d7452ca7b6dbfe506a41a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "81344998d8d34b0d822377d0e47a3e70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b54b8123305f4859bc934357261286ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f5101f3228043049f1c47ed06f965f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94d940b264984e5e86775b2374a1c673": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "824976014c00416d9ddd8392db076872": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06f22992bfc1457ebebaee52bc8773d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0aa1b977ce3c4ec09f48be7ab29928d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86c852f5661c4d76904247328fc21f1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "a046a33c08b0433ea66f126e71e17d0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a70f6bd453e42f995a0ef40c6f22ae2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f26c3fbda5654889a0f061b854ff2063": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10c0fefc702c4f7e808a11a010f4951d",
            "placeholder": "​",
            "style": "IPY_MODEL_803fe311a20a462b8fcb6d5507cc36c8",
            "value": "Connecting..."
          }
        },
        "10c0fefc702c4f7e808a11a010f4951d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "803fe311a20a462b8fcb6d5507cc36c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}